---
title: "Likelihood Intuitively Explained"
author: "Alvaro Gutierrez"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Likeli..what?


First,let's generate a randon sample of 100 observations from normal distribucion with zero mean and one standar deviation.

```{r normal}
set.seed(777) 
n<-10
mu<-0
sigma<-1
norm_data=rnorm(100, mean = mu, sd = sigma)
hist(norm_data, 
     main="Histogram of a Normal Distribution ", 
     xlab="Value of X", 
     border="black", 
     col="white",
     xlim=c(-3,3),
     las=1, 
     breaks=50)
```

Now let's generate a function that will allow us to get the likelihood of a normal distribution.
```{r likelihood_function}
likelihood = function(data, mu, sigma) {
  likelihood = 1
  for(obs in data){
    likelihood = likelihood * (1/(sqrt(2*pi)*sigma)) * exp((-1/2) * ((obs - mu)^2)/(sigma^2))
  }
  return(likelihood)
}
```


```{r likelihood_calculation}
like_norm<-likelihood(norm_data, 0, 1)
like_norm
```
Where this loop is absolutely the same if we type the close form of the likelihood:

\begin{equation}
L(\mu,\sigma;x) = \left( \frac{1}{\sqrt{2\pi\sigma^{2}}} \right)^{N}\cdot e^\left(  {  
	\frac{-\sum_{i=1}^{N}(x_{i} - \mu)^{2}}{2\sigma^{2}}}\right)
\end{equation}

```{r like_close_form}
square_sum = function(data,mu){  
    square_sum = 0  
    for(obs in data){
square_sum = square_sum + ((obs - mu)^2)
    }
return(square_sum)
}
n<-100
mu<-0
sigma<-1

like_close_form<- (1/(sqrt(2*pi*sigma*sigma))^{n})*exp((-square_sum(norm_data,0))/(2*sigma^{2})  )
like_close_form
```




```{r loglikelihood_fuction}
loglikelihood = function(data, mu, sigma) {
  loglikelihood = 0
    for(obs in data){
      loglikelihood = loglikelihood + (-1/2)*log(2*pi*sigma*sigma) - ((obs - mu)^(2))/(2*sigma*sigma)
    }
    return(loglikelihood)
}
```




```{r loglikelihood_calculation}
loglike_norm<-loglikelihood(norm_data, 0, 1)
loglike_norm
log(like_norm)
```



#Resolution of ML : Newton Raphson Method

```{r newton-raphson}
newton <- function(f, tol = 1e-7, x0 = 1, N = 100){
  h <- 1e-7
  i <- 1; x1 = x0
  p <- numeric(N)
  while (i <= N) {
    df.dx <- (f(x0 + h) - f(x0)) / h
    x1 <- (x0 - (f(x0) / df.dx))
    p[i] <- x1
    i <- i + 1
    if (abs(x1 - x0) < tol) break
    x0 <- x1
  }
  return(p[1 : (i-1)])
}
```


This is a function that let us aproximate:

(1) $f$: function that we want to aproximate
(2) $tol$: Toleance level
(3) $x_{0}$: Initial guess
(4) $N$: default number of iterations.

##Simple Example:


```{r example_1 }
f <- function(x){x^5 - 7}
h <- 1e-7
df.dx <- function(x){(f(x + h) - f(x)) / h}
df.dx(1);df.dx(2)
app <- newton(f, x0 = 2)
app
length(app)
app[length(app)]



f(app[length(app)]) #almost zero
```



To corroborate
```{r check }
7^{(1/5)}
```

## Comming back to Normal Distribution: Simply writting the normal function.
```{r example_normal }
mu<-1
sigma<-1

f <- function(x){(1/(sqrt(2*pi*sigma*sigma)))*exp((-(x-mu)^{2})/(2*sigma^{2})  )}
h <- 1e-7
df.dx <- function(x){(f(x + h) - f(x)) / h}
df.dx(1);df.dx(2)
app <- newton(f, x0 = 2)
app
length(app)
app[length(app)]





```


ESTO ESTÁ MAL... LO QUE TENGO QUE OPTIMIZAR ES EL PARÁMETRO DESCONOCIDO MU Y SIGMA!!!!!!!!!!


## Comming back to Normal Distribution: Simply writting the normal function.

```{r example_likelihoodl }
mu<-1
sigma<-1
n<-100
h <- 1e-7
df.dx <- function(x){(f(x + h) - f(x)) / h}
df.dx(1);df.dx(2)
app <- newton(loglikelihood(norm_data,0,1), x0 = 2)
app
length(app)
app[length(app)]

```





#Checking with a preloaded library

```{r gls }
library(nlme)
m1 = gls(norm_data ~ 1, method='ML')
summary(m1)


```


#References
Most of the code was taken from:

[(1)](https://www.r-bloggers.com/maximum-likelihood/) Deatailed explanation of Maximum Likelihood Programming into R with loops.

[(2)](https://www.academia.edu/7031789/Newton-Raphson_Method_in_R?auto=download) Excelente explanation of Newton-Raphson Method for cuadratic functions.

